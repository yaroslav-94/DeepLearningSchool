{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8f64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from matplotlib import colors, pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d99ce8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# разные режимы датасета \n",
    "DATA_MODES = ['train', 'val', 'test']\n",
    "# все изображения будут масштабированы к размеру 224x224 px\n",
    "RESCALE_SIZE = 224\n",
    "# работаем на видеокарте\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f11286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpsonsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Датасет с картинками, который паралельно подгружает их из папок\n",
    "    производит скалирование и превращение в торчевые тензоры\n",
    "    \"\"\"\n",
    "    def __init__(self, files, mode):\n",
    "        super().__init__()\n",
    "        # список файлов для загрузки\n",
    "        self.files = sorted(files)\n",
    "        # режим работы\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode not in DATA_MODES:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
    "            raise NameError\n",
    "\n",
    "        self.len_ = len(self.files)\n",
    "     \n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            self.labels = [path.parent.name for path in self.files]\n",
    "            self.label_encoder.fit(self.labels)\n",
    "\n",
    "#             with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
    "#                   pickle.dump(self.label_encoder, le_dump_file)\n",
    "                      \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        image = Image.open(file)\n",
    "        image.load()\n",
    "        return image\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        # для преобразования изображений в тензоры PyTorch и нормализации входа\n",
    "        transform = transforms.Compose([     \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        x = self.load_sample(self.files[index])\n",
    "        x = self._prepare_sample(x)\n",
    "        x = np.array(x / 255, dtype='float32')\n",
    "        x = transform(x)\n",
    "        if self.mode == 'test':\n",
    "            return x\n",
    "        else:\n",
    "            label = self.labels[index]\n",
    "            label_id = self.label_encoder.transform([label])\n",
    "            y = label_id.item()\n",
    "            return x, y\n",
    "        \n",
    "    def _prepare_sample(self, image):\n",
    "        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n",
    "        return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9226985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20933\n",
      "----------------------------------------------------------------------\n",
      "Общий список всех героев из датасета\n",
      "abraham_grampa_simpson 913\n",
      "agnes_skinner 42\n",
      "apu_nahasapeemapetilon 623\n",
      "barney_gumble 106\n",
      "bart_simpson 1342\n",
      "carl_carlson 98\n",
      "charles_montgomery_burns 1193\n",
      "chief_wiggum 986\n",
      "cletus_spuckler 47\n",
      "comic_book_guy 469\n",
      "disco_stu 8\n",
      "edna_krabappel 457\n",
      "fat_tony 27\n",
      "gil 27\n",
      "groundskeeper_willie 121\n",
      "homer_simpson 2246\n",
      "kent_brockman 498\n",
      "krusty_the_clown 1206\n",
      "lenny_leonard 310\n",
      "lionel_hutz 3\n",
      "lisa_simpson 1354\n",
      "maggie_simpson 128\n",
      "marge_simpson 1291\n",
      "martin_prince 71\n",
      "mayor_quimby 246\n",
      "milhouse_van_houten 1079\n",
      "miss_hoover 17\n",
      "moe_szyslak 1452\n",
      "ned_flanders 1454\n",
      "nelson_muntz 358\n",
      "otto_mann 32\n",
      "patty_bouvier 72\n",
      "principal_skinner 1194\n",
      "professor_john_frink 65\n",
      "rainier_wolfcastle 45\n",
      "ralph_wiggum 89\n",
      "selma_bouvier 103\n",
      "sideshow_bob 877\n",
      "sideshow_mel 40\n",
      "snake_jailbird 55\n",
      "troy_mcclure 8\n",
      "waylon_smithers 181\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = Path(r'D:\\journey-springfield\\train')\n",
    "TEST_DIR = Path(r'D:\\journey-springfield\\test')\n",
    "\n",
    "# Добавляет в переменную файлы из директории\n",
    "train_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\n",
    "test_files = sorted(list(TEST_DIR.rglob('*.jpg')))\n",
    "\n",
    "print(len(train_val_files))\n",
    "# Словарь с уникальными ключами по именам персонажей\n",
    "d_names = {}\n",
    "max_elem_name = ''\n",
    "max_elem_num = 0\n",
    "for i in train_val_files:\n",
    "    if i.parent.name in d_names:\n",
    "        d_names[i.parent.name] += 1\n",
    "    else:\n",
    "        d_names[i.parent.name] = 1\n",
    "    if d_names[i.parent.name]>max_elem_num:\n",
    "        max_elem_name = i.parent.name\n",
    "        max_elem_num = d_names[i.parent.name]\n",
    "\n",
    "print('-'*70)\n",
    "print(\"Общий список всех героев из датасета\")\n",
    "for key, value in d_names.items():\n",
    "    print(key, value)\n",
    "print('-'*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e294f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_labels = [path.parent.name for path in train_val_files]\n",
    "train_files, val_files = train_test_split(train_val_files, test_size=0.25, \\\n",
    "                                          stratify=train_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a1c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    #Ваш код здесь\n",
    "    losses = {'train': [], \"val\": []}\n",
    "    f1_metric = {'train': [], \"val\": []}\n",
    "    accuracy_metric = {'train': [], \"val\": []}\n",
    "\n",
    "    pbar = trange(num_epochs, desc=\"Epoch:\")\n",
    "\n",
    "    for epoch in pbar:\n",
    "\n",
    "        # каждя эпоха имеет обучающую и тестовую стадии\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # установаить модель в режим обучения\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # итерируемся по батчам\n",
    "            for data in tqdm(dataloaders[phase], leave=False, desc=f\"{phase} iter:\"):\n",
    "                # получаем картинки и метки\n",
    "                inputs, labels = data\n",
    "\n",
    "                # оборачиваем в переменные\n",
    "                if use_gpu:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                else:\n",
    "                    inputs, labels = inputs, labels\n",
    "\n",
    "                # инициализируем градиенты параметров\n",
    "                if phase==\"train\":\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                if phase == \"eval\":\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "                else:\n",
    "                    outputs = model(inputs)\n",
    "                preds = torch.argmax(outputs, -1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # backward pass + оптимизируем только если это стадия обучения\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # статистика\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += int(torch.sum(preds == labels.data))\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "           \n",
    "            # Ваш код здесь\n",
    "            losses[phase].append(epoch_loss)\n",
    "            accuracy_metric[phase].append(epoch_acc)\n",
    "\n",
    "            pbar.set_description('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                                    phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # если достиглось лучшее качество, то запомним веса модели\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # загрузим лучшие веса модели\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, losses, accuracy_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62872862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 15699, 'val': 5234}\n",
      "3925\n",
      "1309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "val_dataset = SimpsonsDataset(val_files, mode='val')\n",
    "train_dataset = SimpsonsDataset(train_files, mode='train')\n",
    "\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0),\n",
    "    'val': torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)       \n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(train_dataset),\n",
    "    'val': len(val_dataset)\n",
    "}\n",
    "\n",
    "print(dataset_sizes)\n",
    "print(len(dataloaders['train']))\n",
    "print(len(dataloaders['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a39a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "model = models.alexnet(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96200a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import OrderedDict\n",
    "\n",
    "layers_to_unfreeze = 8\n",
    "\n",
    "# Выключаем подсчет градиентов для слоев, которые не будем обучать\n",
    "for param in model.features[:-layers_to_unfreeze].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n",
    "num_features = 1408\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model._fc = nn.Sequential(OrderedDict([\n",
    "    ('batch_norm', nn.BatchNorm1d(num_features)),\n",
    "    ('drop', nn.Dropout(p=0.5)),\n",
    "    ('linerar1', nn.Linear(num_features, 1000)),\n",
    "#     ('drop', nn.Dropout(p=0.5)),\n",
    "#     ('relu', nn.ReLU()),\n",
    "    ('linerar2', nn.Linear(num_features, len(np.unique(train_val_labels))))\n",
    "]))\n",
    "\n",
    "\n",
    "# Использовать ли GPU\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "# В качестве cost function используем кросс-энтропию\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# В качестве оптимизатора - стохастический градиентный спуск\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Умножает learning_rate на 0.1 каждые 7 эпох (это одна из эвристик, не было на лекциях)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a281d139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6a9f5dd01b493bb0d3d04d1f19082b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch::   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train iter::   0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val iter::   0%|          | 0/1309 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train iter::   0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val iter::   0%|          | 0/1309 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train iter::   0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val iter::   0%|          | 0/1309 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train iter::   0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val iter::   0%|          | 0/1309 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train iter::   0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val iter::   0%|          | 0/1309 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 23m 41s\n",
      "Best val Acc: 0.886320\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9d0df1f6c75f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.autonotebook import trange, tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "model, losses, accuracy_metric = train_model(model, loss_fn, optimizer_ft, exp_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab92b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c59a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f61baa7",
   "metadata": {},
   "source": [
    "8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "# Построим график метрик качества при обучении и валидации\n",
    "\n",
    "#Ваш код здесь\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(accuracy_metric['train'], label=\"train\")\n",
    "plt.plot(accuracy_metric['val'], label=\"val\")\n",
    "plt.plot(accuracy_metric['train'], 'ro')\n",
    "plt.plot(accuracy_metric['val'], 'go')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc41f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "# Построим график лосса при обучении и валидации\n",
    "\n",
    "#Ваш код здесь\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(losses['train'], label=\"train\")\n",
    "plt.plot(losses['val'], label=\"val\")\n",
    "plt.plot(losses['train'], 'ro')\n",
    "plt.plot(losses['val'], 'go')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
